# solutions in distribute envirment

## consistency , availability , partition tolerance

c，一致性被称为原子对象，任何的读写看起来都应该是原则，或串行的。写后面的读一定能读到前面写的内容，所有的读写请求都好像被全局排序。a，对任何非失败节点都应该在有限时间内给出请求的回应（请求的可终止性）。p，允许节点之间丢失任意多的消息，当网络发生分区时，节点之间的消息可能完全丢失。

a关注的是用户对分布式系统的可用要求，p关注的是分布式系统实例间的网络连通性；a从外部视角，要求分布式系统在正常响应时间内一直可用，p从实例节点视角，在遇到某节点或节点间通信故障的时候，要求分布式系统整体对节点的容错及恢复性。

在分布式环境中，多实例部署是基本条件，应为网络的不可靠性，导致p必须满足。所以三选二的组合演化为cp和ap两个分支。cp不是简单的放弃a，而是保障cp的硬性条件去追求a，所以产生了过半写入的使用方式，过半写入后，分布式节点可以根据少数服从多数完成数据的一致性要求，因此产生最大效益：1，分布式实例的更高可用性，对所有实例不在全部写入成功后才认为是成功；2，分布式实例的更快响应性，使用广播快速获取过半结果后直接认定结果，依靠补充手段实现数据的一致性。

ap也不是为了高可用而放弃了数据的一致性。从脏读，幻读来说，场景允许数据的短暂不一致，接收数据的最终一致性。数据的严谨性是系统的一个基本要求，但允许数据的一定延迟是ap存在的意义，系统的高可用可以满足更多的群体，从这个目标上说，ap是比较友好的。

在分布式环境下，对[cap](https://mp.weixin.qq.com/s/3fK7ScVhUGWoTHZ9eBVyJQ)的要求，不管是ap还是cp，并不是完全丢弃另一个，而是优先级问题，在满足c或者a的基础上去追求另外一个。cp，在满足强一致性的要求上去追求a，如过半写入；ap在高可用的基础上去追求数据的一致性，如最终一致性。总的来说，系统以ap为基调，在一些数据一致性要求高的场景用cp进行补充。

## quorum

quorum是抽屉原理的一个应用。假设有n个副本，更新操作在w个副本中更新成功之后，才认为此次更新操作成功。对于读操作而言，至少需要读r个副本才可以读到此次更新的数据，其中w+r>n，即w和r有重叠，一般：w+r=n+1。

hdfs的namenode采用qjm机制（[Quorum Journal Manager](https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)）实现hdfs的ha(high availability)。为了实现ha，需要两台namenode机器，一台是active namenode，负责client请求；一台是standby namenode，负责与active namenode数据同步，从而快速failover。数据同步用到了一个三方集群：journal nodes。active namenode和standby namenode都和journal nodes通信，从而实现同步。

每次namenode写editlog时，除向本地磁盘写入外，也会并行向journal nodes集群中的每一个journal node发送写请求，只要大多数的节点返回成功，就认为向集群写入成功。如果有2n+1台节点，最多可容忍n台节点挂掉。

[Hadoop NameNode 高可用 (High Availability) 实现解析](https://developer.ibm.com/zh/articles/os-cn-hadoop-name-node/)

## paxos

paxos算法是一种基于消息传递的一致性算法（一致性算法有两种实现方式：通过基于锁的共享内存和消息传递），用来解决在分布式系统中的数据一致性问题。paxos的思想是基于quorum（法定人数）机制，少数服从多数，对于少数节点的网络异常，宕机，数据不一致问题等，不重要，消息尽一切努力送达，数据达到最终一致。

在一个决议提议的过程中，其他决议会被否决，否决本身意味着更多的网络io，意味着更多冲突，这些冲突需要额外的开销，代价较大。为解决类似问题，zk对paxos协议进行改进，提出zab协议。zab协议把整个过程分为两个部分，1，选总体；2，进行决议。

[phxpaxos](https://github.com/Tencent/phxpaxos/blob/master/README.zh_CN.md) | [拜占庭模型](https://www.jianshu.com/p/a79d362de138) 

## raft

[raft算法](https://www.jdon.com/artichect/raft.html)

## consistent hashing

一致性哈希算法是一种分布式哈希实现算法，提出了在动态变化的cache环境中，判断哈希算法好坏的四个标准：1，平衡性(balance)，哈希的结果能够尽可能分布到所有的缓存中去，可以使所有的缓存空间都得到利用；2，单调性(monotonicity)，如果有一些内容已经通过哈希分派到了相应的缓存中，又有新的缓存加入到系统中，哈希的结果应该能够保证原有已分配的内容可以被映射到原有或新的缓存中，而不会被映射到旧的缓存中其他的缓存中；3，分散性(spread)，在分布式环境中，终端有可能看不到所有的缓存区，而只能看到其中一部分，当终端希望通过哈希将内容映射到缓存上时，由于不同终端所见的缓存范围有可能不同，从而导致哈希结果不一致，最终相同的内容被映射到不同的缓冲区上，分散性是指上面情况发生的严重程度，好的哈希算法应尽量避免不一致情况发生，也就是尽量降低分散性；4，负载（load），是从另一个角度看待分散性，既然不同的终端可将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同用户映射为不同内容，这种情况应当也是避免的。

环形哈希空间，将obj通过特定的hash函数计算出对应的key值，然后散列到hash环上，然后将机器通过hash算法映射到环上，对象和机器处于同一hash空间中，按照顺时针转动，obj存储到下一个机器中。普通hash求余算法最大的问题就是在有机器添加或者删除之后，造成大量的存储位置失效，不能满足单调性，而一致哈希则有效规避了这个问题。一致哈希算法通过引入虚拟节点来满足平衡性。

## distribute transcation

分布式事务实现方案从类型上分：刚性事务，柔性事务。刚性事务指通常无业务改造，强一致性，原生支持回滚/隔离性，低并发，适合短事务。柔性事务，有业务改造，最终一致性，实现补偿接口，实现资源锁定接口，高并发，适合长事务。刚性事务有，[XA协](https://mp.weixin.qq.com/s/HUT_sGvIzoOjpVEONKy1Gg)议（[2pc](https://mp.weixin.qq.com/s/KlwOKC4XDJduUckDD4OhVg)， jta， jts），[3pc](https://mp.weixin.qq.com/s/Mb-sR70b-59DWqe6Bw3dfg)；柔性事务有，[tcc](https://mp.weixin.qq.com/s/e1tPlNVYgPVVAnbY2Tm3DQ)/fmt，[saga](https://mp.weixin.qq.com/s/HDSWK2eCOtusroV3Elv1jA)（状态机模式，aop模式），本地事务消息，[消息事务（半消息）](https://mp.weixin.qq.com/s/Vbff2ABVT1yqpBh_bxaVMA)，[最大努力通知事务消息](https://mp.weixin.qq.com/s/jDZ2HtkqiXRf9DKxNuDpGw)。

2pc是xa规范实现思路，xa规范是x/open dtp定义的交易中间件与数据库之间的接口规范，交易中间件用它来通知数据库事务的开始，结束，提交及回滚等。xa模型包括应用程序ap，事务管理器tm，资源管理器rm，通信资源管理器crm四部分，一般tm为交易中间件，rm为数据库，crm为消息中间件。

2pc通常使用到xa的tm，ap，rm。ap发起事务，通常为微服务自身，定义事务的边界，并访问事务边界内的资源；tm为事务协调方，事务操作总控，管理全局事务，分配事务唯一标识，监控事务执行进度，负责事务的提交，回滚，失败恢复等。rm本地事务资源，跟进协调方命令进行操作，管理本地共享资源（本地数据库）。

2pc分两个阶段，请求阶段和提交阶段。请求阶段tm串行给每个rm发送prepare消息，rm要么直接返回失败，要么在本地执行sql，记录事务日志（undo，redo），但不提交。提交阶段，如果tm收到了rm的失败消息或超时，直接给所有rm发送rollback消息；否则，发送commit消息，rm跟进tm的指令执行提交或者回滚操作。最后，释放所有事务处理过程中使用的锁资源。

2pc存在的问题：1，全流程同步阻塞，不管1阶段还是2阶段，所有参与节点都为事务阻塞型，当参与者占有公共资源时，其他三方访问公共资源可能不得不处于阻塞状态；2，tm单点故障，由于全流程依赖tm协调，一旦tm发生故障，参与者会一直阻塞下去，尤其在第二阶段，tm发生故障，所有参与者都处于锁定事务资源的状态中，而无法继续完成事务，所有参与者必须等tm重新上线才可继续工作；3，tm脑裂引起数据不一致，二阶段中，当tm向rm发送commit请求之后，发生了局部网络异常，或者在发送commit请求过程中tm发生故障，将会导致只有一部分rm接收到了commit请求，这部分rm接收到请求之后，执行commit，但其他未接收到commit请求的rm则无法执行事务提交，于是整个分布式系统出现了数据不一致；4，tm脑裂引起的事务状态不确定，tm在发出commit消息之后宕机，而接收到消息的rm同时也宕机了，那么即使通过选举协议产生了新的tm，事务的状态也是不确定的。

3pc是解决2pc的数据不一致和事务状态不确定问题而出现，3pc确保任何分支下的数据一致性，确保任何分支最多3次握手得到最终结果（超时机制），rm超时后的事务状态必须从tm获取，2pc只有tm超时机制，3pc增加了rm超时机制，一方面辅助解决2pc的事务问题，另外还能降低一定的同步阻塞问题。因为tm，rm的双向超时机制，所以3pc又被定义为“非阻塞”协议。

3pc分为三个阶段：准备阶段cancommit，对齐阶段precommit，提交阶段docommit。准备阶段，tm向rm发送commit请求，rm如果可以提交返回yes，否则no，询问超时默认rm未no，准备阶段只做sql处理，不记录事务日志。对齐阶段，tm通知各个rm事务最终状态，各个rm如果一致未收到事务对齐通知，会在超时后从tm反查事务状态，实现事务状态对齐，然后记录事务日志redo和undo。提交阶段，根据对齐阶段得到的事务状态结果，各rm根据tm的命令进行提交/abort或者超时后自动提交/abort。

分布式事务的触发场景大约有：1，跨数据库，数据库的物理分割下保障跨库操作的acid；2，跨服务，服务的网络分割下保障多服务的事务完整性。解决方案总体有：业务规避>base柔性事务>cp刚性事务。刚性事务首先有xa规范，xa规范的实现方案是2pc，开源框架有atomikos，bitronix，seata xa模型及各大数据库厂商对xa的落地。xa规范定义将事务的参与者分成了tm，ap和rm，并要求事务资源，如数据库，本身提供对规范和协议的支持，这样做的好处：1，可以从任何角度保证数据的隔离；2，实现数据的全局一致；3，对业务无侵入。

xa规范将事务机制落地到事务资源上面，使用的是xaconnection和xadatasource，两者均属于数据库驱动范围。如果框架自己实现，无法保证数据的兼容完整性，最佳实践需要数据资源，如数据库，本身提供对规范和协议的支持，seata xa模型也是如此。xa框架通过connection通讯层面去处置事务机制，避免sql相关处理，也利用了数据库内部xa实现，因此最能保证数据全局一致性。但也有如下缺陷：1，数据锁定，数据在事务未结束前，为了保障一致性，根据数据的隔离基本进行锁定；2，协议阻塞，本地事务在全局事务没有commit或callback前都是阻塞等待的；3，性能损耗高，主要体现在事务协调增加的rt（retry）成本，并发事务数据使用锁进行竞争阻塞。

刚性事务都是cp的，所以不可避免有性能上限，无法满足超高量级的互联网场景。因此出现了柔性事务，相对数据库的acid刚性事务，柔性事务本质是对xa协议的妥协，通过降低强一致性要求，从而降低数据库资源的锁定时间，提升可用性。其实就是基于base理论，保证数据的最终一致。

basically available，基本可用；soft state，柔性状态；eventually consistency最终一致性。基本可用指分布式系统出现不可知故障时，允许损失部分可用性。具体为，一响应时间的损失，通过延长响应时间（服务重试其他提供者）来保障可用性；二功能上的损失，流量高峰时，通过服务降级，限流等治理手段提供有损服务。柔性状态指允许系统的数据存在中间状态，并认为中间状态的存在不影响系统的整体可用，即允许系统在不同节点的数据副本之间进行数据同步的过程存在数据延时。最终一致性指系统中所有的数据副本，在经过一段时间的数据同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

柔性事务主要分为补偿型和通知型，补偿型又分为tcc，saga；通知型分mq事务消息，最大努力通知型。补偿型事务都是同步的，通知型则是异步的。

tcc是一种补偿型事务，它要求应用的每个服务提供try，confirm，cancel三个接口，其核心思想是通过对资源的预留（提供中间态，如账户状态、资金冻结等），尽早释放对资源的加锁，如果事务可以提交，则完成对预留资源的确认，如果事务要回滚，则释放预留资源。tcc模型完全交由业务实现，每个子业务都需要实现try，confirm，cancel三个接口，对业务侵入大，资源锁定交由业务方。1，try，尝试执行业务，完成所有业务检查（一致性），预留必要的业务资源（准隔离性）；2，confirm，确认执行业务，不再做业务检查，只使用try阶段预留的业务资源，confirm操作满足幂等性；3，cancel，取消执行业务，释放try阶段预留业务资源。

2pc的操作对象在资源层，对开发人员无感知，而tcc的操作在于业务层，具有较高的开发成本；2pc是一个整体的长事务，也是刚性事务，而tcc是一组本地短性事务，是柔性事务；2pc的prepare表决阶段，进行了操作表决，而tcc的try阶段并没有表决准备，直接兼备资源操作与准备能力；2pc是全局锁定资源，所有参与者阻塞，交互等待tm通知，而tcc的资源锁定在于try操作，业务方可灵活选择业务资源的锁定粒度。

tcc为了解决网络不可靠引起的异常情况，要求业务方在设计上要遵循三个策略。1，允许空回滚，原因为异常发生在阶段一时，部分参与方没有收到try请求从而触发整个事务的cancel操作，try失败或者没有执行try操作的参与方收到cancel请求时，要进行空回滚操作；2，保持幂等性，原因是异常发生在第二阶段时，比如网络超时，则会重复调用参与方的confirm/cancel方法，因此confirm/cancel方法必须保持幂等性；3，防止资源悬挂，原因网络异常导致两个阶段无法保证严格的顺序执行，出现参与方侧try请求比cancel请求更晚到达的情况，cancel会执行空回滚而确保事务的正确性，但是此时try方法也不可以再被执行。

tcc对业务的侵入性强，使用成本非常昂贵，虽然提供了更灵活的资源锁粒度，比2pc有更高的吞吐量，当相对于2pc的强一致性，tcc的实施成本和数据一致性的牺牲带来的相对高吞吐量，总体表现出来的性价比较低，所以在成熟大厂中几乎没有使用。

saga模型是把一个分布式事务拆分为多个本地事务，每个本地事务都有相应的执行模块和补偿模块（对应tcc的confirm和cancel），当saga事务中任意一个本地事务出错时，可以调用相关的补偿方法恢复之前的事务，达到事务最终一致性。saga模型主要分，1，一串子事务（本地事务）的事务链；2，每个saga子事务tn都有对应的补偿定义cn用于撤销tn造成的结果；3，每个tn都没有预留动作，直接提交到库。执行顺序为t1-tn，或者中间j出错后撤销之前所有的t。数据的隔离性，通过业务层控制并发，在应用层加锁，应用层预先冻结资源等。恢复方式有，1，向后恢复，如任一子事务失败，则补偿所有已完成的事务；2，向前恢复，重试失败的事务，假设每个子事务最终都会成功。从saga模型定义中，其可以满足事务的三个特性，1，原子性，saga协调器协调事务链中的本地事务，要么全部提交，要么全部回滚；2，一致性，saga事务可以实现最终一致性；3，持久性，基于本地事务实现。saga无法保证外部的原子性和隔离性因为可以查看其他sagas的部分结果。

saga和tcc一样，都是强依靠业务改造，同样要求业务方在设计上遵循：允许空补偿，保持幂等性，防止资源悬挂；和tcc比，saga没有try，直接commit，所以会留下原始事务操作的痕迹，cancel属于不完美补偿，需要考虑对业务的影响。tcc的cancel是完美补偿的rollback，补偿操作会彻底清理之前的原始事务操作，用户是感知不到事务取消之前的状态信息。saga的补偿操作通常可以异步执行，tcc的cancel和confirm可以根据需要异步化。saga对业务侵入较小，只需要提供一个逆向操作的cancel即可，而tcc需要对业务进行进行全局性的流程改造。tcc最少通信次数为2n，而saga为n（n为子事务数量）。

目前业界saga的实现方式有：1，基于业务逻辑层proxy设计，基于aop实现，比如华为的servicecomb；2，状态机实现的机制，比如阿里的seata的saga模式。

aop proxy实现原理如下，业务逻辑层调用上加上事务注解`@Around("execution(* *(..)) &&* @annotation(TX)")`，proxy在真正业务逻辑被调用之前，生成一个全局唯一的 **txid** 标识事务组，txid保存在threadlocal变量里面，方法开始前写入，完成后清除，并向远端数据库写入txid，把事务组置为开始状态。业务逻辑层调用数据访问层之前，通过rpc proxy代理记录当前调用请求请求参数。如果业务正常，调用完成后，当前方法的调用记录存档或者删除，如果业务异常，查询调用链反向补偿。数据访问层设计，原始接口必须保证幂等性，提供补偿接口实现反向操作。这方面可以在框架层面做一些通用补偿实现，降低使用成本，补偿接口也必须是有幂等性保证。还可以提供补偿注解，基于原则接口方法，在方法名加注解标准补偿方法名：`@Compensable(cancelMethod='cancelRecord')`。补偿策略，首先调用执行失败，修改事务组状态；其次分布式事务补偿服务异步执行补偿。

状态机引擎[saga](https://mp.weixin.qq.com/s/HDSWK2eCOtusroV3Elv1jA)原理为，流程为先执行stateA，再执行stateB，然后stateC。状态执行是基于事件驱动的模型，stateA执行完成后，会产生路由消息放入EventQueue，事件消费端从EventQueue取出消息，执行stateB，在整个状态机启动时，会调用seata server开启分布式事务，并产生xid，然后记录“状态机实例”启动事件到本地数据库。当执行到一个“状态”时会调用seata server注册分支事务，并产生branchid，然后记录“状态实例”开始执行事件到本地数据库。当一个状态执行完成后，会记录“状态实例”执行结束事件到本地数据库，然后调用seata server上报分支事务的状态。当整个状态机执行完成，会记录“状态机实例”执行完成事件到本地数据库，然后调用seata server提交或者回滚分布式事务。

柔性事务包含补偿型事务和通知型事务。上面刨析了tcc和saga中的补偿事务。通知型事务主要包含事务消息和最大努力通知型分布式事务。通知型事务的核心思想是通过mq来通知其他事务参与者自己事务的执行状态。mq组件的引入有效将事务参与者解耦，各个参与者都可以异步执行，所以通知型事务又称异步事务。事务消息的难度在于服务本地事务和投递消息的一致性保障。目前业界的解决这个一致性的方案有：1，基于mq自身的事务消息方案；2，基于db的本地消息表方案。

基于mq的事务消息方案主要依靠mq的半消息机制来实现投递消息和参与者自身本地事务的一致性保障。半消息机制实现原理其实借鉴2pc的思路，是二阶段提交的广义拓展。其流程为：1，事务发起方首先发送prepare消息到mq；2，在发送prepare消息成功后，执行本地事务；3，根据本地事务执行结果返回commit或rollback；4，如果消息为rollback，mq将删除该prepare消息不进行下发，如果是commit，则mq将消息发送给consumer；5，如果执行本地事务过程中，执行端挂掉，或者超时，mq服务器端将不停询问producer来获取事务状态；6，consumer端的消费成功由mq保障。mq事务消息因为使用了半消息机制，对业务有较大侵入性。需注意：1，业务方调用半消息，并提供对应的回查方法；2，mq要提供半消息机制，并定期扫描长期半消息，对消息生产者进行回查确认事务；3，消费方需进行幂等消费。

有时我们的mq组件并不支持事务消息，或者我们想尽量减少业务侵入，我们可基于db本地消息表。1，业务方直接利用本地事务，将业务数据和事务消息直接写入数据库；2，投递线程，使用专门的投递线程进行事务消息投递到mq，根据投递ack去删除事务消息表记录。本地事务消息表的优势在于方案的通用性，无需提供回查方法，进一步减少业务侵入。在某些场景下，还可以利用注解等形式解耦，有可能实现无业务代码侵入式的实现。以下为一个企业级事务消息设计流程：1，事务消息服务，提供通用投递接口，用于保障事务消息的本地写入，并将事务消息写入事务内存队列；2，使用投递线程池，继续事务内存队列投递派发分配，投递工作线程只投递本实例拥有的事务消息，投递失败线程列入时间轮队列，重试机制使用失败挡位区分，默认提供6挡：5s、10s、15s、20s、25s、30s；3，时间轮线程进行60s转动，将到期的失败事务消息重入事务内存队列；4，因为我们的事务消息服务是无状态化的多实例存在，所以需要一个持锁线程进行主节点竞争抢锁，处理一些额外工作；5，因为我们的事务内存队列是内存级别的，不可避免面临重启等情况下的数据丢失，这时需要事务消息服务主节点进行定期扫表，将长期未投递的事务消息取出放入事务消息服务；6，事务消息服务主节点还有一个清理线程，专门用于将已处理成功的历史事务消息进行归档清理，降低db的数据量。

mq事务消息方案，需要mq支持半消息机制或者类似特性，在重复投递上具有较好的去重处理；需要业务方进行改造，提供对应的本地操作成功回查功能，具有较大的业务侵入。db本地事务消息表方案，使用数据库来存储事务消息，降低了对mq的需求，但增加了存储成本；事务消息使用了异步投递，增大了消息重复投递的可能性。两种事务消息的共性有，1，事务消息都依赖mq进行事务通知，所以都是异步的；2，事务消息在投递方都是存在重复投递的可能，需要有配套机制去降低重复投递率，实现更优好的消息投递去重；3，事务消息的消费方，因为投递重复的无可避免，因此需要进行消费去重设计或者服务幂等设计。

最大努力通知事务，其实现仍是基于mq进行事务控制。最大努力通知事务和事务消息都是通知型事务，主要适用于需要异步更新的数据，并对数据的实时性要求较低的场景。最大努力主要用于外部系统，因为外部网络更加复杂和不可信，故只能尽最大努力去通知实现数据的最终一致。而事务消息主要适用于内部系统的数据最终一致性保证，因为内部相对比较可控。两者的核心要点一致：保证参与者事务和消息的一致性投递。最大努力通知在消息的投递上面也有：1，基于mq自身的事务消息方案；2，基于db的本地事务消息表。

最大努力通知事务在投递之前和消息事务类似，关键在投递后处理，由于事务消息在于内部的事务处理，所以mq和系统是直连，并且无需严格的权限，安全等方面的思路设计。最大努力则在于第三方系统的对接，所以最大努力通知事务有以下的特性：1，业务主动方在完成业务处理后，向业务被动方（第三方系统）发送通知消息，允许存在消息丢失；2，业务主动方提供递增多挡位时间间隔（5min，10min，30min，1h，24h），用于失败重试调用业务被动方接口，在通知n次后不再通知，报警记录日志人工介入；3，业务被动方提供幂等的服务接口，防止通知重复消费；4，业务主动方需要有定期校验，对业务数据进行兜底，防止业务被动方无法履行责任时进行业务回滚，确保数据最终一致性。

从参与者来说，最大努力通知事务适用于跨平台，跨企业的系统间业务交互，事务消息更适用于同网络体系的内部服务交付；从消息层面来说，最大努力通知事务需要主动推送并提供多档次时间的重试机制来保证数据的通知，而事务消息只需要消息消费者主动消费；从数据层面来说，最大努力通知事务还需要额外的定期校验机制对数据进行兜底，保证数据的最终一致性，而事务消息只要保证消息的可靠投递即可，自身无需对数据进行兜底处理。最大努力通知事务本质是通过引入定期校验机制来对最终一致性兜底，对业务侵入较低，适合于对最终一致性敏感度比较低，业务链路较短的场景。

[saga](https://www.jianshu.com/p/e4b662407c66?from=timeline&isappinstalled=0) | [2pc&3pc](https://www.jianshu.com/p/dd6a340e50b2) | [tcc](https://www.cnblogs.com/jajian/p/10014145.html) | [xa](https://blog.csdn.net/wuzhiwei549/article/details/79925618) | [可靠消息最终一致性](https://www.cnblogs.com/haizai/p/11954339.html) | [seata设计原理](https://yq.aliyun.com/articles/715556?spm=a2c4e.11157919.spm-cont-list.23.3b31f2047Bulkr) | [seata](https://github.com/seata/seata) | [txc](http://arick.net/content/46)

## 分布式锁

redission的实现思路，client加锁时，如果是redis集群，会首先hash到一台机器，然后发送一段lua脚本（保证操作原子性）：

```java
if (redis.call('exists',keys[1]) == 0) then
    redis.call('hset',keys[1],argv[2],1);
    redis.call('pexpire',keys[1],argv[1]);
    return nil;
end;
if(redis.call('hexists',keys[1],argv[2] ==1) then
    redis.call('hincrby',keys[1],argv[2],1);
    redis.call('pexpire',keys[1],argv[1]);
    return nil;
end;
return redis.call('pttl',keys[1]);
```

其中，keys[1]为要加的锁，如'SyncTaskLock'，argv[1]为锁的默认生存时间，默认30秒，argv[2]为加锁的客户端id。

上面的逻辑为，如果锁不存在，则加锁，通过hset lock完成，设置一个hash的数据结构，并设置超时时间。如果另外一个client尝试加锁，发现lock已经存在，然后判断lock的hash中是否存在client2的id不存在，则返回client2一个数字，pttl lock代表lock剩余生存时间。此后，client2进入一个循环，不停尝试加锁。而client1一但加锁成功，则会启动一个watch dog每隔10s检查一下，如果client1还持有锁，则不断延长锁的生存时间。如果client1已经持有锁但又想重复加锁，则进入第二个if，通过hincrby增client1加锁的次数。当释放锁的时候，则每次对加锁的次数减一，如果发现加锁次数为0，则删除lock，然后client2就可以尝试加锁了。

如果对某个redis master写入了锁，此时会异步复制给对应的master slave，但该过程中一旦发生master宕机，主备切换，就会导致client2在新的master上加锁成功client1也加锁成功。

curator基于zk实现，锁是zk上的一个节点，zk的节点分为四种，持久节点，持久顺序节点，临时节点和临时顺序节点。分布式锁用到了临时顺序节点。当a和b对zk发起加锁请求时，两者都在锁节点下创建一个临时顺序节点。假如a先发起，创建完临时节点后，a会检查下lock节点下的所有子节点，如果a是第一个创建节点的，那么a加锁成功。当b来尝试加锁时，也会先创建一个临时顺序节点，b也会检查lock节点下的所有子节点，发现之前a已经创建过了，则加锁失败，之后b会对a创建的节点添加一个监听器，监听这个节点是否被删除。a释放锁的时候，删除自己创建的临时节点，zk通知监听这个节点的监听器，也就是b，b会重新尝试去获取锁。采用临时节点，如果某个客户端创建临时顺序节点之后，不小心宕机，zk会感知到那个客户端宕机，自动删除对应的临时顺序节点。

## 服务注册，zk，eureka，consul，nacos

cap理论，一致性c，所有的节点在同一时间具有相同的数据；可用性a，保证每个请求无论成功或失败都有响应；分区容忍，系统中任意信息的丢失或失败不影响系统的正常运行；a保证系统本身对外可用，而p则保证系统本事的正常运行。cap三者只能同时满足其中的两者，不能三者同时满足。

zk满足cp，不能保证a。对于数据存储的场景，一致性应该是首先被保证的，但对服务发现却不是，不同节点存储的服务提供信息不相同，也不会有太严重的后果。zk在进行leader选举的时候，服务是不可用的。

eureka则满足ap，不同于zk选举leader的过程，eu采用的p2p对等通信，是去中心化的架构，无master/slave之分，每个peer是对等的。
节点通过彼此相互注册来提高可用性，每个节点需要添加一个或多个有效的serviceurl指向其他节点。每个节点都可被是为其他节点的副本。集群中如果某台server宕机，则client的请求会自动切换到新的server节点上，当宕机的server恢复后，eureka会再次将其纳入到服务器集群管理之中。当节点开始接收client请求时，所有的操作都会在节点间进行复制，将请求复制到该server当前所知的其他所有节点中。当一个新的server启动后，会首先尝试从邻近节点获取所有注册列表信息。并通过geteurekaserviceurls方法获取所有的节点，通过心跳契约定期更新。server在一定时间没有接收到某个服务实例的心跳（默认30秒）将会注销该实例（通过eureka.instance.lease-expiration-duration-in-seconds配置）。server节点端时间丢失过多心跳，将进入自我保护模式。eureka集群只要还有一台server还在，就能保证注册服务可用，虽然可能不是最新的。

eureka的另外一种自我保护机制，如果15分钟内超过85%的节点都没有正常心跳，那么eureka就认为client与注册中心出现了网络故障。eureka不再从注册表中移除因长时间没有收到心跳而过期的服务；eureka仍能够接收新服务注册和查询请求，但不会被同步到其他节点上；当网络稳定时，当前实例新注册的信息会被同步到其他节点中。

consul内置服务注册与发现，分布一致性协议实现，健康检查，kv存储，多数据中心方案等。遵循cp，保证强一致性和分区容错性，使用了raft算法。consul的强一致性导致服务注册比eureka慢，因为raft协议要求必须过半数的节点都写入成功才认为注册成功，leader挂掉时，重新选举期间整个consul不可用。相对，eureka则保证高可用和最终一致性。服务注册较快，因为不需要等注册信息replicate到其他节点，也不保证replicate成功。当数据不一致时，虽然a，b节点的注册信息不完全相同，但每个eureka仍然能够正常对外提供服务，但会出现查询服务信息时，请求a查询不到，但请求b就可以。

nacos是阿里开源，支持基于dns和基于rpc的服务发现。nacos除了服务的注册发现之外，还支持动态配置服务，可以让你以中心化，外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置消除了配置变更时重新部署应用和服务的需要。nacos=springcloud注册中心+springcloud配置中心。

参考：

- [jianshu](https://www.jianshu.com/p/bfcc8855f3d4)
- [nacos](https://nacos.io/zh-cn/docs/quick-start.html)
