# 分布式一致性算法raft

分布式系统中，一致性算法至关重要。Paxos是一种基于消息传递的一致性算法。实现的有Chubby、libpaxos等。zk采用ZAB(zookeeper atomic broadcast)也是基于Paxos算法的，但zab主要用于构建一个高可用的分布式数据主备系统，而Paxos则用于构建一个分布式的一致性状态机系统。相较于Paxos，Raft通过逻辑分离使其更容易理解和实现，实现的有etcd、Consul。

一个raft集群包含若干节点，节点有三种状态：Leader、Follower、Candidate。Leader负责日志同步管理，处理来自客户端的请求，与Follower保持心跳联系。Follower响应Leader的日志同步请求，响应Candidate邀票请求，以及转发客户端请求给Leader。Candidate负责选举投票，集群启动或者Leader宕机时，Follower转为Candidate发起选举，胜出后转为Leader。

为简化逻辑和实现，raft将一致性问题分解为三个相对独立的子问题。选举：当集群初创或leader宕机，一个新的Leader需要被选举出来。日志复制：Leader接收来自客户端的请求，并将其以日志条目的形式复制到集群中的其他节点，并强制要求其他节点的日志和自己的保持一致。安全性：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。

## Leader election

集群启动时，所有节点状态都是Follower，初始Term为0，同时启动选举定时器，每个节点的选举定时器超时时间在100-500毫秒直接并且不一致，避免同时发起选举。由于没有Leader，节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为Candidate，Term自增，并向集群中所有节点发送投票请求并重置选举定时器。

节点收到投票请求，如果：1，请求节点的Term大于自己的Term，且自己尚未投票给其他节点，则接受请求，把票投给它；2，请求节点的Term小于自己的Term，且自己尚未投票，则拒绝请求，将票投给自己。一轮选举过后，正常情况下，会有一个Candidate收到超过半数节点的投票，它将胜出成为Leader。然后定时发送心跳给其他节点，其他节点转为Follower并与Leader保持同步，本轮选举结束。

有可能一轮选举中，没有Candidate收到超过半数节点的投票，那么将进行下一轮。

## Log replication

集群中只有Leader能接受客户端请求，Follower重定向请求给Leader。每一个请求都包含一条被复制状态机执行的指令。Leader把该指令作为一条新的日志条目附加到日志中，然后并行的将附加条目发送给Followers，让它们复制这条日志条目。当该条日志条目被Followers安全的复制，Leader会应用这条日志到它的状态机中，然后返回执行结果给客户端。如果Follow崩溃，运行缓慢，网络丢包，Leader会不断重试直到所有Followers都最终存储了所有的日志条目，确保强一致性。

在发送日志条目的时候，Leader会把新的日志条目紧接着之前的条目的索引位置和Term包含在里面，如果Follower在日志中找不到包含相同索引位置和Term的条目，那么就拒绝，说明Leader和Follower是不一致的。要恢复一致，Leader必须找到最后达成一致得地方，然后删除从那个节点之后的所有日志条目，发送自己的日志给Follower，所有这些都在附加日志的一致性检查时完成。

Leader针对每一个Follower维护了一个nextIndex，表示下一个需要发送给Follower的日志条目的索引地址。当一个Leader刚获得权力时，初始化所有的nextIndex值为自己的最后一条日志的index+1。如果一个Follower的日志和Leader不一致，那么在下一次附加日志时一致性检查就会失败。在被Follower拒绝之后，Leader会减小该Follower对应的nextIndex值并进行重试。

最终Leader会在某个nextIndex和Follower达成一致，从而附加日志成功。这时就会把Follower冲突的日志全部删除并加上Leader的日志。从而Leader和Follower保持一致。

Leader收到客户端请求后，会将它作为日志条目写入本地，该条目的状态为uncommitted。Follower收到同步日志请求后，也为未提交状态。Follower会向Leader发出回应，当Leader收到大多数Followers的回应后，会将日志标记未提交状态，并把日志应用到状态机中。Leader回应客户端后，将随着下一个心跳通知Followers，并也标记日志为提交状态。至此，raft集群中超半数几点达到一致状态，可确保强一致性。那些由于网络、性能、故障导致的反应慢，不一致的问题，也会最终和Leader达成一致。


## 安全性

如果一个follower与leader状态不一致，follower不可用，然后follower恢复可用而leader故障，如果该follower被选举为Leader并覆盖这些日志条目，就会出现不同的状态机执行不同的命令序列。鉴于此，在leader选举的时候需要增加限制，这些限制可保证任何leader对于给定的term，都拥有之前term的所有被y提交的日志条目，即所谓的leader完整性。

### 选举限制

对于所有基于leader机制的一致性算法，leader都必须存储所有已提交的日志条目。raft使用一种简单有效的方法，保证所有之前term中已经提交的日志在选举的时候都会出现在新的leaderh中。日志条目的传送时单向的，只从leader传给follower，并且leader从不会覆盖自身本地日志中已存在的条目。

raft使用投票的方式来阻止一个candidate赢得选举，除非它包含了所有已经提交的日志条目。candidate为了赢得选举必须联系集群中的大部分节点，意味着每一个已经提交的日志条目必须存在于至少一个节点上。如果candidate的日志和大多数的节点一样新，那么它一定持有了所有已经y提交的日志。投票请求中包含了日志信息，然后投票人会拒绝那些日志没有自己新的投票请求。

raft通过比较两份日志中最后一条日志条目的索引值和任期号确定谁的日志比较新。如果两份日志最后的term不同，那么term大的日志更新。如果term相同，那么日志长的更新。

leader知道一条当前term内的日志时可以提交的，只要它被复制到了大多数节点上。如果一个leader在提交日志前崩溃，继任的leader会继续尝试复制这条日志记录。然而，一个leader并不能断定一个之前term里面的日志条目被保存到大多数follower上就一定已经提交了。

鉴于上述情况，raft不会通过计算副本数目的方式去提交一个之前term的日志条目。只有leader当前任期里的日志条目通过计算副本数目可以被提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志的匹配特性，之前的日志条目也都会被间接提交。在某些情况下，leader可以安全知道一个老的日志条目是否已经被提交（只需判断该条目是否存储到所有节点），但raft为了简化问题而使用了一种更保守的方法。

当leader复制之前term的日志时，raft会为所有日志保留原始的任期号，这在提交规则上产生了额外的复杂性。但这种策略更加容易辨别出日志，因为它可以随时间和日志的变化对日志维护着同一个任期编号。此外，该策略是的leader只需要发送较少日志条目。


## etcd

[etcd](https://etcd.io/)是一个高可用，强一致的风不是键值数据库，主要用于共享配置和服务发现，内部使用raft算法作为分布式一致性协议。

lease机制，即租约ttl机制。etcd可以为存储的kv对设置租约，当租约到期，kv将失效删除，同时也支持续约，通过客户端可以在租约到期之前续约，避免kv过期失效；此外还支持解约，一旦解约，与该租约绑定的kv将失效删除。

prefix机制，即前缀机制，也称目录机制，如两个key命名如下：k1=key/k1,k2=key/k2，则可通过前缀/key查询，返回包含两个kv的列表。

watch机制，即监听机制，支持watch某个固定的key，也支持一个范围（前缀），当被watch的key或者范围发生变化，客户端将收到通知。

revision机制，每个key带有一个revision，每进行一次事务加一，因此它是全局唯一的，如初始值为0，进行一次put，revision变为1，同样的操作再进行一次，变为2，换成key1进行put操作，revision变为3。通过revision大小就可以知道进行写操作的顺序，这对于实现公平锁，队列十分有益。

### 应用场景：服务发现

服务发现要解决的是分布式系统中最常见的问题之一，即再同一个分布式集群中的进程或服务如何才能找到对方并能建立连接。需要存在一个高可靠、高可用的中心配置节点，etcd天然支持。服务提供方持续向配置节点注册服务，用户可以在etcd中注册服务，并对服务设置租约，定时续约以达到维持服务的目的，一旦停止续约，服务就会失效。服务调用方会持续地读取中心配置节点地配置并修改本机配置，然后reload服务，服务提供方在etcd制定目录下注册服务，调用方则在对应目录下查询服务。通过watch，调用方还可以监听服务地变化。

### 消息发布和订阅





## ref

[基于etcd的分布式锁](https://www.toutiao.com/a6708613773353026051/?tt_from=weixin&utm_campaign=client_share&wxshare_count=1&timestamp=1561982858&app=news_article&utm_source=weixin&utm_medium=toutiao_android&req_id=2019070120073801015203021635050E9&group_id=6708613773353026051)
