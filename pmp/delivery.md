# delivery

数据调研发生在：1，售前阶段；2，项目建设阶段；售前阶段，也必须进行一定深入程度的数据调研，这样才能明确数据相关的工作量、大体技术方案、确定人力计划和费用概算。数据调研本质上是一个摸家底的过程，搞清楚业主的数据家底，解决方案和费用概算才能相对准确。

sm原则上不参与国家机密中标识为绝密的项目；信息安全规范也明确规定，但凡在项目交付过程中，客户口头或书面明确是绝密的事项，项目交付人员（含驻场）需立即终止服务，上报公司，等待公司的决策。

在项目生命周期，铁三角（AR、FR、SR）都是项目的负责人，在不同阶段，各个角色的作用不一样。当然，在项目进入到交付阶段，项目经理（FR）是主要负责人，或者说第一责任人，但不是唯一负责人。所以项目经理在交付阶段需要拉通销售、售前等资源时，是理直气壮的。

技术中心拿到人口库标准交付件后仍需要数据服务人力；人口库标准交付方案必须基于数梦DataRiver产品才可以正常交付；人口库标准交付的计费单位是字段，而不是人月;人口库标准化交付所用的数据服务人力，应该比传统方式大幅降低（具体说来，数据接入的人力是不会减少的，但是数据治理的工作量，比如说数据探查、整理元数据、清洗、构建主题的工作量可以大幅降低，这些是建立在需求相对标准的情况下）。而且标准化交付的主要目的并不是提升准确性，而是智力资产复用。


项目经理是项目信息安全的第一责任人，负责项目中整体的安全管理工作。

在应急的数据治理的交付中，根据当前方案，服务总线（CSP）的作用是：进行实时数据的接入；对外提供数据服务；对外提供其他应用的服务；实时数据以CSP发布接口并调用消息队列的方式（应急里面是用的消息队列是kafka），将数据导入资源池。这个设计里面当然有一个因素，就是消息队列并不适合直接对外暴漏（网络联通、安全性等各种考虑），需要一个服务总线（或者说API网关）转接一下。在应急部，每天实时数据调用CSP的次数是百万级。而库表数据，一般是通过DataBridge或者离线文件方式接入的，和CSP没有关系。


项目经理在项目开展的初期应该做哪些和信息安全相关的工作，《驻场开发项目信息安全管理规范》培训，签署《项目信息安全学习记录表》； 项目全员签署《项目保密协议》；

产品声明支持的功能规格在项目现场使用出现问题，驻场和一线服务人员需要提交ITSM工单，寻求二线和研发解决；无论使用ITSM工单系统还是dodo需求电子流，都需要详细描述问题出现的场景，现象，以及希望得到的效果，有助于开发准确的定位问题或者进行功能设计；一般的原则是：BUG提交工单，需求提交需求电子流。对于产品的新功能、易用性一般属于新需求范围。

对于删除（或者覆盖）用户文件或数据的敏感操作；提前做好预案；项目经理确认后，客户书面确认；做好备份（无论是文件还是数据库）；（如果是运维期，还需要走变更流程，如果是建设期不需要）；操作，有问题则回退，或者恢复备份；

一个典型的人口库项目，整体交付的工作量，一般由：软件部署、安装、调试、冒烟测试的工作量；来源数据归集或者数据准备的工作量；人口库标准交付方案的工作量；数据门户定制的开发工作量；人口库相关应用开发（含大屏）的开发工作量；人口库标准交付方案无法覆盖的需求，从而产生的其他工作量；



完整的工作量评估应该包括：
1、数据调研（售前和交付阶段都有）
2、项目管理（一般1个人或者兼任）
3、软件部署相关工作量（DataRiver必备，也可能有Bridge、CSP、Catalog）
4、数据归集和准备（这个可能由其他公司完成，如果是这种情况，那么就是确认下其他公司的工作成果）
5、人口库标准交付包（包括几个人月的数据服务，对交付包进行处理操作）
6、数据门户的定制开发（门户网站的定制，不一定有，但是很可能有）
7、相关应用和大屏（不一定有，但是很大可能有。即使是其他厂家开发，也有一定的专题数据开发和对接工作量）
8、人口库标准化交付是针对字段，在标准覆盖范围外，可能有一些需求，如果有的话，会有工作量。



在驻场开发的情况下，仅项目经理或开发经理（如果有此角色）能拥有项目代码的全局访问权限，其他项目成员只能访问自己工作相关部分的代码。


按照数梦的数据治理体系，数据资源池分成原始层、资源层、主题层、专题层存放;资源层是原始层经过标准化、清洗、去重等操作后的数据，表结构和原始层大体一致;主题层需要通过抽象和建模的方法提炼模型;原始层的数据原则上保留的，并不是转换成资源层后就丢弃;在数据治理实践中，各层的数据都有可能对外提供服务。当然，直接调用原始层的数据是比较少的，毕竟还没有怎么经过治理。但是其他三层都有可能;



关于DataRiver数据标准化清洗,数据标准化操作主要包括：标准清洗、去重、去空;清洗动作中的字典映射标准化，可以把源系统业务记录中字典字段清洗成标准字典数据，反过来也可以把标准字典数据映射成源系统业务中字典数据;字典表映射管理可以通过字典表管理页面（“数据地图--标准清洗--数据元清洗--字典表”）来管理系统字典，也可以通过导入导出的方式在文件中维护;去重操作是很消耗系统资源的操作，River实现该动作的技术方式有“MapReduce”和“SparkSQL”实现两种方式，SparkSQL性能会好一些,但是毕竟是内存，可能存在不能适用的场景，如超大数据量。所以至少目前看来，MapReduce方式也没有要淘汰的说法。


当一个select语句同时出现了where, group by, having, order by的时候，正确的执行顺序是:.Where,Group By,Having,Order by

代码、开发设计文档等机密信息提供给用户及合作伙伴应走:Dodo ->  信息安全  -> 对外披露流程

GAAP完成确认有哪几个必要的前提:合同完成签订，销售订单已下;项目拿到数梦签约方的验收报告;如果数梦做总集，还需要完成数梦和ISV的验收;总集为数梦关联运营公司或者 非运营商 过单，需要穿透验收;涉及合同范围变更，销管要完成订单修改

一般来说，衡量数据质量的指标有哪几个:数据量;数据一致性;数据是否完整;数据更新频率;数据是否足够准确

目前人口库标准化交付的“标准”体现在哪几个方面:std层的结构跟ods保持一致，属于项目定制加工，目前未做标准化;dws层是根据客户具体需求进行开发，也未作标准化;目前标准化的部分有dwd层，以及数据元和数据字典。除此之外，还有这些标准字段的加工逻辑。

DataRiver可通过哪些方式进行建表:可视化模式建表;DDL模式建表; 建模方式建表;

ETL的过程包括:数据抽取;数据转换;数据装载;ETL和ELT的不同在于，数据转换是在装载之前还是装载之后。对照数梦的产品，如果转换是在DataBridge之中完成，那么就是ETL；如果是装载完成后，通过DataRiver进行数据转换，那么就是ELT。当然，这两个步骤并不矛盾，所以也可以是ETLT。


关于数据库分区下列说法正确的是:数据库分区的目的是为了在特定SQL操作中减少数据读写的总量以缩短响应时间;.数据仓库不同层的分区策略有所不同，ODS层一般以数据集成导入时间作为分区，DWD层通常选取业务字段作为分区键;数据量较小的表通常不采用分区策略;

关于全量数据抽取原则的描述，正确的是:一般针对数据量较小的表；针对没有时间戳字段并且不能通过redolog等日志方式获取增量数据的表；抽取后数据应该按LOAD_DATE加载日期分区进行保存

发生安全事件时，项目组成员需及时报告给：1、项目信息安全负责人（项目经理）、2、流程与IT工程部。因为需要公司需要及时评估性质和影响，项目组也需要及时获取公司的支持和救济，尽快止损。


关于公司Bridge支持CDC数据增量导入说法正确的:仅支持Oracle、MySQL、SQLServer  三种数据库CDC增量导入;CDC数据源包括以下四种，Oracle  CDC、Oracle  Redolog、SQL  Server  CDC、mysqlbinlog;CDC模式数据增量导入到EMR后必须按照数据加载时间做去重操作;

docker rm 是删除容器;docker ps 是显示容器信息;docker rmi 是删除镜像（在删除镜像之前，必须先删除相关容器；除非使用-f参数强制删除）;docker system prune 用来删除不再使用的 docker 对象（会删除所有未被 tag 标记和未被容器使用的镜像）


